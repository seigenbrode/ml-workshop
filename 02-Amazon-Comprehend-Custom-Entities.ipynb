{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1099b04396475b6a0143fa303da9fa44ad87b660"
   },
   "source": [
    "# Amazon Comprehend - Custom Entity Recognition\n",
    "\n",
    "\n",
    "**Description:** This lab is walks you through the steps required to prepare a dataset and submit a custom entity recognizer for Amazon Comprehend\n",
    "\n",
    "More information on how to create a custom entity recognizer model can be found here.\n",
    "\n",
    "   https://docs.aws.amazon.com/comprehend/latest/dg/training-recognizers.html\n",
    "\n",
    "\n",
    "*Note: This notebook and content has been created using content from the following sources and adapted for this workshop.*  \n",
    "   - [Amazon Comprehend Custom Workshop](https://github.com/aws-samples/amazon-comprehend-custom-entity)\n",
    "   - [AWS Blog - Build a custom entity recognizer using Amazon Comprehend](https://aws.amazon.com/blogs/machine-learning/building-a-custom-classifier-using-amazon-comprehend/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Before you start, make sure that your SageMaker Execution Role has the credentials that will be required for this lab.  First, grab the SageMaker Execution Role attached to this session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print('SageMaker Execution Role: ', role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required Credentials**\n",
    "\n",
    "Open the [IAM Console - Roles](https://console.aws.amazon.com/iam/home?region=us-east-1#/roles), search for the role above and ensure that role has the required credentials listed below. , search for the role above and ensure that role has the required credentials listed below. \n",
    "\n",
    "  (1) Comprehend Full Access\n",
    "  \n",
    "  (2) SageMaker Full Access\n",
    "  \n",
    "  (3) S3 Full Access \n",
    "  \n",
    "  (4) You will need to create and add an inline policy allowing iam:PassRole as shown below:\n",
    "  \n",
    "             {\n",
    "               \"Version\": \"2012-10-17\",\n",
    "               \"Statement\": [\n",
    "                {\n",
    "                 \"Action\": [\n",
    "                 \"iam:PassRole\"\n",
    "                 ],\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Resource\": \"*\"\n",
    "               }\n",
    "              ]\n",
    "             }\n",
    "             \n",
    "   (5) You will also need to add the following trust policies to your IAM Role:\n",
    "   \n",
    "           {\n",
    "             \"Version\": \"2012-10-17\",\n",
    "             \"Statement\": [\n",
    "             {\n",
    "               \"Effect\": \"Allow\",\n",
    "               \"Principal\": {\n",
    "                 \"Service\": [\n",
    "                   \"sagemaker.amazonaws.com\",\n",
    "                   \"s3.amazonaws.com\",\n",
    "                   \"comprehend.amazonaws.com\"\n",
    "                 ]\n",
    "               },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "             }\n",
    "            ]\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import additional libraries we will be using for the lab...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import botocore\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "comprehend = boto3.client('comprehend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set your S3 bucket and prefix...**\n",
    "\n",
    "In this case we will be using our default session bucket for simplicity.  This bucket will be used for our model data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sess.default_bucket()\n",
    "prefix = 'comprehend-custom-entity'\n",
    "print('S3 Bucket for our model data: ', bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "54e810d8b9c1936c8569093badabc4d7b25ea881"
   },
   "source": [
    "In this example we will be using the following twitter dataset which contains tweets to and from companies doing customer support on twitter: https://www.kaggle.com/thoughtvector/customer-support-on-twitter\n",
    "\n",
    "**Download the dataset above and save it in the ./data folder on this notebook instance**\n",
    "\n",
    "*Note: If you don't have an account on kaggle you can run the following commands from the notebook terminal to copy the dataset to your notebook instance:* \n",
    "\n",
    "   aws s3 cp s3://phi-demo-london/twcs/twcs.zip /home/ec2-user/SageMaker/ml-workshop/data/twcs.zip\n",
    "\n",
    "   cd /home/ec2-user/SageMaker/ml-workshop/data\n",
    "\n",
    "   unzip twcs.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's explore our data a bit by loading it into a DataFrame...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9365c16e4481ec49f5c084f7c3b0cf50dd55047f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('./data/twcs.csv',encoding='utf-8')\n",
    "print(tweets.shape)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "845eba8749f15e1e2b10aa43414f40860259f4e0"
   },
   "source": [
    "The schema for the dataset above includes: \n",
    "\n",
    "  (1) **tweet_id:** Unique ID for this tweet\n",
    "  \n",
    "  (2) **author_id:** Unique ID for this tweet author (anonymized for non-company users)\n",
    "  \n",
    "  (3) **inbound:** Indicates whether the tweet was sent inbound to a company\n",
    "  \n",
    "  (4) **created_at:** When the tweet was created\n",
    "  \n",
    "  (5) **text:** Text content of the tweet\n",
    "  \n",
    "  (6) **response_tweet_id:** The unique ID of the tweet that responded to this tweet\n",
    "  \n",
    "  (7) **in_response_to_tweet_id:** The tweet this tweet was in response to\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5e053048057a5566a30aab3f0278aa529449938a"
   },
   "source": [
    "### Data Wrangling\n",
    "\n",
    "This is a very interesting tweet data set, about 3 million tweets, and we have information on the author of the tweets and whether the tweet was a query or a response (the \"inbound\" column). If the tweet was a query, the response_tweet_id gives the response made by the support team.\n",
    "\n",
    "It would be interesting to modify this dataframe to get query - response pairs in every row.\n",
    "The following code, to do just what we want, was pulled from [this kernel](https://www.kaggle.com/soaxelbrooke/first-inbound-and-response-tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "first_inbound = tweets[pd.isnull(tweets.in_response_to_tweet_id) & tweets.inbound]\n",
    "\n",
    "QnR = pd.merge(first_inbound, tweets, left_on='tweet_id', \n",
    "                                  right_on='in_response_to_tweet_id')\n",
    "\n",
    "# Filter to only outbound replies (from companies)\n",
    "QnR = QnR[QnR.inbound_y ^ True]\n",
    "print(f'Data shape: {QnR.shape}')\n",
    "QnR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0428e41c670dbe801090613580cf22e3b41723b5"
   },
   "outputs": [],
   "source": [
    "#Let's filter the dataframe contains only the needed columns\n",
    "QnR = QnR[[\"author_id_x\",\"created_at_x\",\"text_x\",\"author_id_y\",\"created_at_y\",\"text_y\"]]\n",
    "QnR.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to only telco tweets\n",
    "In our example, we want to create a custom entity to recognize smartphones devices. Let's filer our dataframe to only incclude the T-Mobile and Sprint tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_telco = QnR[QnR[\"author_id_y\"].isin([\"TMobileHelp\", \"sprintcare\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's concatenate the question and response into one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_telco['text'] = tweet_telco['text_x'] + ' | ' + tweet_telco['text_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our telco tweets as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweet_telco['text'].to_csv('./data/tweet_telco.csv', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity list\n",
    "In order to create our dataset we need to provide an entity list for our new class named DEVICE.\n",
    "\n",
    "For now, in order to create our entity list, we will generate keywords of different smartphones manually. The list includes unique entities that have at least 1000 matches in our training dataset.\n",
    "\n",
    "*Note: In the interest of time, we are only executing one notebook that is part of a larger workshop for [Comprehend Custom](https://github.com/aws-samples/amazon-comprehend-custom-entity).  We'd encourage those interested in exploring Comprehend Custom further to check out the [second notebook](https://github.com/aws-samples/amazon-comprehend-custom-entity/blob/master/2-BlazingText-Word2Vec-Telco-tweets.ipynb) in that workshop where we load a corpus into a word2vec model and generate a list of keywords that are contextually similar. This technique will be used in the custom classifer in the third notebook. The same technique could alternatively be applied here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sphones = ['iPhone X', 'iPhoneX', 'iphoneX', 'Samsung Galaxy', 'Samsung Note', 'iphone', 'iPhone', 'android', 'Android']\n",
    "\n",
    "df_entity_list = pd.DataFrame(sphones, columns=['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's add another column with our class label. This is required part of the Amazon Comprehend training dataset.\n",
    "\n",
    "More information can be found here.\n",
    "\n",
    "https://docs.aws.amazon.com/comprehend/latest/dg/cer-entity-list.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entity_list['Type'] = 'DEVICE'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entity_list.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's create our training, entity list, and test file and upload it to S3...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "training_file = './data/telco_train.csv'\n",
    "tweet_telco['text'].to_csv(training_file, encoding='utf-8', index=False)\n",
    "\n",
    "entity_file = './data/entity_list.csv'\n",
    "df_entity_list.to_csv(entity_file, encoding='utf-8', index=False)\n",
    "\n",
    "test_file = './data/telco_device_test.csv'\n",
    "tweet_telco['text'].tail(10000).to_csv(test_file, encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(s3path, file):\n",
    "    s3 = boto3.resource('s3')\n",
    "    data = open(file, \"rb\")\n",
    "    key = s3path\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)\n",
    "\n",
    "s3_train_key = prefix + \"/train/telco_train.csv\" \n",
    "s3_test_key = prefix + \"/test/telco_device_test.csv\"\n",
    "s3_entity_key = prefix + \"/entity/telco_entity.csv\"\n",
    "\n",
    "upload_to_s3(s3_train_key, training_file)\n",
    "upload_to_s3(s3_test_key, test_file)\n",
    "upload_to_s3(s3_entity_key, entity_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create s3 paths variable \n",
    "s3_train_data = 's3://{}/{}'.format(bucket, s3_train_key)\n",
    "s3_train_entity = 's3://{}/{}'.format(bucket, s3_entity_key)\n",
    "s3_test_data = 's3://{}/{}'.format(bucket, s3_test_key)\n",
    "s3_output_test_data = 's3://{}/{}/test/{}'.format(bucket, prefix, \"telco_test_output.json\")\n",
    "print('uploaded training data location: {}'.format(s3_train_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Let's prepare the Custom Entity training job request file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_entity_request = {\n",
    "\n",
    "      \"Documents\": { \n",
    "         \"S3Uri\": s3_train_data\n",
    "      },\n",
    "      \"EntityList\": { \n",
    "         \"S3Uri\": s3_train_entity\n",
    "      },\n",
    "      \"EntityTypes\": [ \n",
    "         { \n",
    "            \"Type\": \"DEVICE\"\n",
    "         }\n",
    "      ]\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id = str(datetime.datetime.now().strftime(\"%s\"))\n",
    "create_custom_entity_response = comprehend.create_entity_recognizer(\n",
    "        RecognizerName = \"custom-device-recognizer\"+id, \n",
    "        DataAccessRoleArn = role,\n",
    "        InputDataConfig = custom_entity_request,\n",
    "        LanguageCode = \"en\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobArn = create_custom_entity_response['EntityRecognizerArn']\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_custom_recognizer = comprehend.describe_entity_recognizer(\n",
    "        EntityRecognizerArn = jobArn\n",
    "    )\n",
    "    status = describe_custom_recognizer[\"EntityRecognizerProperties\"][\"Status\"]\n",
    "    print(\"Custom entity recognizer: {}\".format(status))\n",
    "    \n",
    "    if status == \"TRAINED\" or status == \"IN_ERROR\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the different metrics for our custom entity recognizer. Amazon Comprehend provides you with metrics to help you estimate how well an entity recognizer should work for your job. They are based on training the recognizer model, and so while they accurately represent the performance of the model during training, they are only an approximation of the API performance during entity discovery.\n",
    "\n",
    "More information can be found here: https://docs.aws.amazon.com/comprehend/latest/dg/cer-metrics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(describe_custom_recognizer[\"EntityRecognizerProperties\"][\"RecognizerMetadata\"][\"EntityTypes\"], indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our output above we can evaluate our model for common metrics: \n",
    "\n",
    " (1) **Precision:** This indicates how many times the model makes a correct entity identification compared to the number of attempted identifications. This shows how many times the model's entity identification is truly a good identification. It is a percentage of the total number of identifications.\n",
    " \n",
    " (2) **Recall:** This indicates how many times the model makes a correct entity identification compared to the number of instances of that the entity is actually present (as defined by the total number of correct identifications true positives (tp) and missed identifcations false negatives (fn).\n",
    " \n",
    " (3) **F1:** This is a combination of the Precision and Recall metrics, which measures the overall accuracy of the model for custom entity recognition. The F1 score is the harmonic mean of the Precision and Recall metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our custom entity model\n",
    "\n",
    "Let's invoke the Comprehend API to run our test job from the test file we prepared earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_response = comprehend.start_entities_detection_job(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': s3_test_data,\n",
    "        'InputFormat': 'ONE_DOC_PER_LINE'\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Uri': s3_output_test_data\n",
    "    },\n",
    "    DataAccessRoleArn=role,\n",
    "    JobName='Custom_Device_Test',\n",
    "    EntityRecognizerArn=jobArn,\n",
    "    LanguageCode='en'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's monitor the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobId = test_response['JobId']\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_job = comprehend.describe_entities_detection_job(\n",
    "        JobId = jobId\n",
    "    )\n",
    "    status = describe_job[\"EntitiesDetectionJobProperties\"][\"JobStatus\"]\n",
    "    print(\"Job Status: {}\".format(status))\n",
    "    \n",
    "    if status == \"COMPLETED\" or status == \"FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the test output to local machine\n",
    "job_output = describe_job[\"EntitiesDetectionJobProperties\"][\"OutputDataConfig\"][\"S3Uri\"]\n",
    "path_prefix = 's3://{}/'.format(bucket)\n",
    "job_key = os.path.relpath(job_output, path_prefix)\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(bucket).download_file(job_key, 'output.tar.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xvzf output.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all the Entities values in a list\n",
    "import json\n",
    "\n",
    "data = []\n",
    "for line in open('output', 'r'):\n",
    "    entities = json.loads(line)['Entities']\n",
    "    if entities != None and len(entities) > 0:\n",
    "        data.append(entities[0]['Text'])\n",
    "    \n",
    "\n",
    "# function to get unique values \n",
    "def unique(list1): \n",
    "      \n",
    "    # insert the list to the set \n",
    "    list_set = set(list1) \n",
    "    # convert the set to the list \n",
    "    unique_list = (list(list_set)) \n",
    "    for x in unique_list: \n",
    "        print(x), \n",
    "        \n",
    "unique(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the list of the above entities that were recognized with the manual entity list we created and used as  input to our training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entity_list.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results above, we're able to see that Comprehend Custom Entity Recognition was able to recognize entities based on the list we created for training but you'll also notice that Amazon Comprehend has picked up additional words with varying spellings, which is something that can be expected when analyzing data that has typos or abbreviated spellings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONGRATULATIONS! \n",
    "You've successfully created a Custom Entity Recognizer using Amazon Comprehend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
